{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mark von Oven - Homework 2\n",
    "## Data Analytics & Machine Learning at Scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW2.0:\n",
    "### How do you merge  two sorted  lists/arrays of records of the form [key, value]? Where is this  used in Hadoop MapReduce? [Hint within the shuffle]\n",
    "### What is  a combiner function in the context of Hadoop? \n",
    "### Give an example where it can be used and justify why it should be used in the context of this problem.\n",
    "### What is the Hadoop shuffle?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HW2.0:\n",
    "When you pass records in [key,value] form to the reducer, Hadoop merges them automatically for you.\n",
    "The combiner function is an aggregator that can be invoked before sending records to the reducer.  This can be helpful when there is a large amount of information, and combining it can preserve network bandwidth.\n",
    "The Hadoop shuffle is what occurs when Hadoop is preparing the map outputs to be sent to the reducer(s) based on output keys. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW2.1: Counters as a debugging aid (and for getting work done, but please use sparingly as they are heavy)\n",
    "\n",
    "Consumer complaints dataset: Use Counters to do EDA (exploratory data analysis and to monitor progress)\n",
    "Counters are lightweight objects in Hadoop that allow you to keep track of system progress in both the map and reduce stages of processing. By default, Hadoop defines a number of standard counters in \"groups\"; these show up in the jobtracker webapp, giving you information such as \"Map input records\", \"Map output records\", etc. \n",
    "\n",
    "While processing information/data using MapReduce job, it is a challenge to monitor the progress of parallel threads running across nodes of distributed clusters. Moreover, it is also complicated to distinguish between the data that has been processed and the data which is yet to be processed. The MapReduce Framework offers a provision of user-defined Counters, which can be effectively utilized to monitor the progress of data across nodes of distributed clusters.\n",
    "\n",
    "Use the Consumer Complaints  Dataset provide here to complete this question:\n",
    "\n",
    "     https://www.dropbox.com/s/vbalm3yva2rr86m/Consumer_Complaints.csv?dl=0\n",
    "\n",
    "Use the following command to grab the file:\n",
    "\n",
    "    curl -L https://www.dropbox.com/s/vbalm3yva2rr86m/Consumer_Complaints.csv?dl=0 -o Consumer_Complaints.csv\n",
    "\n",
    "The consumer complaints dataset consists of diverse consumer complaints, which have been reported across the United States regarding various types of loans. The dataset consists of records of the form:\n",
    "\n",
    "Complaint ID,Product,Sub-product,Issue,Sub-issue,State,ZIP code,Submitted via,Date received,Date sent to company,Company,Company response,Timely response?,Consumer disputed?\n",
    "\n",
    "Here’s is the first few lines of the  of the Consumer Complaints  Dataset:\n",
    "\n",
    "Complaint ID,Product,Sub-product,Issue,Sub-issue,State,ZIP code,Submitted via,Date received,Date sent to company,Company,Company response,Timely response?,Consumer disputed?\n",
    "1114245,Debt collection,Medical,Disclosure verification of debt,Not given enough info to verify debt,FL,32219,Web,11/13/2014,11/13/2014,\"Choice Recovery, Inc.\",Closed with explanation,Yes,\n",
    "1114488,Debt collection,Medical,Disclosure verification of debt,Right to dispute notice not received,TX,75006,Web,11/13/2014,11/13/2014,\"Expert Global Solutions, Inc.\",In progress,Yes,\n",
    "1114255,Bank account or service,Checking account,Deposits and withdrawals,,NY,11102,Web,11/13/2014,11/13/2014,\"FNIS (Fidelity National Information Services, Inc.)\",In progress,Yes,\n",
    "1115106,Debt collection,\"Other (phone, health club, etc.)\",Communication tactics,Frequent or repeated calls,GA,31721,Web,11/13/2014,11/13/2014,\"Expert Global Solutions, Inc.\",In progress,Yes,\n",
    "\n",
    "### User-defined Counters\n",
    "\n",
    "Now, let’s use MapReduce Counters to identify the number of complaints pertaining to debt collection, mortgage and other categories (all other categories get lumped into this one) in the consumer complaints dataset. Basically produce the distribution of the Product column in this dataset using counters (limited to 3 counters here).\n",
    "\n",
    "Hadoop offers Job Tracker, an UI tool to determine the status and statistics of all jobs. Using the job tracker UI, developers can view the Counters that have been created. Screenshot your  job tracker UI as your job completes and include it here. Make sure that your user defined counters are visible. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting MRcomplaint_counter.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile MRcomplaint_counter.py\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "\n",
    "class MRComplaintCount(MRJob):\n",
    "    def mapper(self, _, line):\n",
    "        complaint_id, complaint_type, _ = line.split(',', 2)\n",
    "        if complaint_type != \"Product\":\n",
    "            if complaint_type in [\"Debt collection\", \"Mortgage\"]:\n",
    "                self.increment_counter(complaint_type, 'Num_mapper_calls', 1)\n",
    "                yield complaint_type, 1\n",
    "            else:\n",
    "                self.increment_counter(\"Other\", 'Num_mapper_calls', 1)\n",
    "                yield \"Other\", 1\n",
    "\n",
    "    def reducer(self, word, counts):\n",
    "        self.increment_counter(word, 'Num_reducer_calls', 1)\n",
    "        yield word, sum(counts)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MRComplaintCount.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'Debt collection': {'Num_mapper_calls': 44372, 'Num_reducer_calls': 1}, 'Other': {'Num_mapper_calls': 142788, 'Num_reducer_calls': 1}, 'Mortgage': {'Num_mapper_calls': 125752, 'Num_reducer_calls': 1}}]\n"
     ]
    }
   ],
   "source": [
    "#HW2.1\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from MRcomplaint_counter import MRComplaintCount\n",
    "mr_job = MRComplaintCount(args=['Consumer_Complaints.csv'])\n",
    "with mr_job.make_runner() as runner: \n",
    "    runner.run()\n",
    "    # stream_output: get access of the output \n",
    "    print runner.counters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW2.2: Analyze the performance of your Mappers, Combiners and Reducers using Counters\n",
    "\n",
    "For this brief study the Input file will be one record (the next line only): \n",
    "foo foo quux labs foo bar quux\n",
    "\n",
    "### Perform a word count analysis of this single record dataset using a Mapper and Reducer based WordCount (i.e., no combiners are used here) using user defined Counters to count up how many time the mapper and reducer are called. What is the value of your user defined Mapper Counter, and Reducer Counter after completing this word count job. The answer  should be 1 and 4 respectively. Please explain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!echo foo foo quux labs foo bar quux > WordCount.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mr_wc_counter.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mr_wc_counter.py\n",
    "#HW2.2\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import re\n",
    " \n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    " \n",
    "class MRWordFreqCount(MRJob):\n",
    "    def mapper(self, _, line):\n",
    "        self.increment_counter('group', 'Num_mapper_calls', 1)\n",
    "        for word in WORD_RE.findall(line):\n",
    "            yield word.lower(), 1\n",
    "\n",
    "    def reducer(self, word, counts):\n",
    "        self.increment_counter('group', 'Num_reducer_calls', 1)\n",
    "        yield word, sum(counts)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MRWordFreqCount.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'group': {'Num_mapper_calls': 1, 'Num_reducer_calls': 4}}]\n"
     ]
    }
   ],
   "source": [
    "#HW2.2\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from mr_wc_counter import MRWordFreqCount\n",
    "mr_job = MRWordFreqCount(args=['WordCount.txt'])\n",
    "with mr_job.make_runner() as runner: \n",
    "    runner.run()\n",
    "    # stream_output: get access of the output \n",
    "    print runner.counters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HW2.2:\n",
    "The mapper is only called once because there is only 1 line being analyzed...the line and the code is sent to a single mapper to be processed.\n",
    "The reducer is called four times because there are four unique keys (i.e. words) in the output of the mapper (i.e. foo, quux, labs, bar).  All keys containing a unique word are sent to the same reducer and then reduced to a single version of key, sum(value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform a word count analysis of the Issue column of the Consumer Complaints Dataset using a Mapper and Reducer based WordCount (i.e., no combiners used anywhere) using user defined Counters to count up how many times the mapper and reducer are called. What is the value of your user defined Mapper Counter, and Reducer Counter after completing your word count job. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mr_CCC_counter.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mr_CCC_counter.py\n",
    "#HW2.2\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    " \n",
    "class MRWordFreqCount(MRJob):\n",
    "    def mapper(self, _, line):\n",
    "        c_id, c_type, sub_prod, issue, _ = line.split(',', 4)\n",
    "        if issue != \"Issue\":\n",
    "            self.increment_counter('group', 'Num_mapper_calls', 1)\n",
    "            yield issue, 1\n",
    "\n",
    "    def reducer(self, word, counts):\n",
    "        self.increment_counter('group', 'Num_reducer_calls', 1)\n",
    "        yield word, sum(counts)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MRWordFreqCount.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'group': {'Num_mapper_calls': 312912, 'Num_reducer_calls': 80}}]\n"
     ]
    }
   ],
   "source": [
    "#HW2.2\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from mr_CCC_counter import MRWordFreqCount\n",
    "mr_job = MRWordFreqCount(args=['Consumer_Complaints.csv'])\n",
    "with mr_job.make_runner() as runner: \n",
    "    runner.run()\n",
    "    # stream_output: get access of the output \n",
    "    print runner.counters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HW2.2 The mapper is called 312912 times and the reducer 80 times.\n",
    "[{'group': {'Num_mapper_calls': 312912, 'Num_reducer_calls': 80}}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform a word count analysis of the Issue column of the Consumer Complaints Dataset using a Mapper, Reducer, and standalone combiner (i.e., not an in-memory combiner) based WordCount using user defined Counters to count up how many time the mapper, combiner, reducer are called. What is the value of your user defined Mapper Counter, combiner counter, and Reducer Counter after completing your word count job. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing mr_CCCCCC_counter.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mr_CCCCCC_counter.py\n",
    "#HW2.2\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    " \n",
    "class MRWordFreqCount(MRJob):\n",
    "    def mapper(self, _, line):\n",
    "        c_id, c_type, sub_prod, issue, _ = line.split(',', 4)\n",
    "        if issue != \"Issue\":\n",
    "            self.increment_counter('group', 'Num_mapper_calls', 1)\n",
    "            yield issue, 1\n",
    "\n",
    "    def combiner(self, key, values):\n",
    "        self.increment_counter('group', 'Num_combiner_calls', 1)\n",
    "        yield key, sum(values)\n",
    "    \n",
    "    def reducer(self, word, counts):\n",
    "        self.increment_counter('group', 'Num_reducer_calls', 1)\n",
    "        yield word, sum(counts)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MRWordFreqCount.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'group': {'Num_combiner_calls': 147, 'Num_mapper_calls': 312912, 'Num_reducer_calls': 80}}]\n"
     ]
    }
   ],
   "source": [
    "#HW2.2\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from mr_CCCCCC_counter import MRWordFreqCount\n",
    "mr_job = MRWordFreqCount(args=['Consumer_Complaints.csv'])\n",
    "with mr_job.make_runner() as runner: \n",
    "    runner.run()\n",
    "    # stream_output: get access of the output \n",
    "    print runner.counters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW2.2.1: \n",
    "### Using a single reducer perform a sort of the words in decreasing order of word counts. Present the top 50 terms and their frequency. If there are ties please sort the tokens in alphanumeric/string order. Present bottom 10 tokens (least frequent items). \n",
    "\n",
    "HINT: You will need a second MRStep for the sort part. Step 1 will be the usual word count, while step 2 will be a sort step. Please use the Hadoop/MRJob framework to perform the sort. Please do NOT use any of the built-in sorts  from  python.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting MRWordFreqCountSort.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile MRWordFreqCountSort.py\n",
    "#HW2.2.1\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import sys, re, string, operator\n",
    "\n",
    "class MRWordFreqCountSort(MRJob):\n",
    "    \n",
    "    def get_words(self, _, line):\n",
    "        regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "        line = regex.sub(' ', str(line).lower())\n",
    "        line = re.sub( '\\s+', ' ', line )\n",
    "        \n",
    "        words = line.split()\n",
    "        for w in words: \n",
    "            yield(w, 1)\n",
    "\n",
    "    def sum_words(self, word, values):\n",
    "        yield(word, sum(values))\n",
    "        \n",
    "    def sort_words(self, word, value):\n",
    "        newkey = '{number:012d}'.format(number=int(value))+word\n",
    "        yield(newkey, [word, value])\n",
    "\n",
    "    def red_init(self):\n",
    "        self.d=[]\n",
    "    \n",
    "    def reduce_words(self, key, values):\n",
    "        for value in values:\n",
    "            self.d.append([value[0], value[1]])\n",
    "            \n",
    "    def reduce_words_final(self):\n",
    "        index = len(self.d)-1\n",
    "        bottom = 10\n",
    "#        while index >=0:                    #values in desc order\n",
    "#            yield(self.d[index][0], self.d[index][1])\n",
    "#            index -= 1\n",
    "#        for key, value in self.d:          #if i want the values in asc order\n",
    "#            yield(key, value)\n",
    "        print(\"***Top 50***\")\n",
    "        for i in range(50):\n",
    "            print(self.d[index - i][0], self.d[index - i][1])\n",
    "        print(\"***Bottom 10***\")\n",
    "        for j in range(10):\n",
    "            print(self.d[j][0], self.d[j][1] )\n",
    "        \n",
    "    def steps(self):\n",
    "        return [MRStep(mapper=self.get_words,\n",
    "                      reducer=self.sum_words),\n",
    "                MRStep(mapper=self.sort_words,\n",
    "                      reducer_init=self.red_init,\n",
    "                      reducer=self.reduce_words,\n",
    "                      reducer_final=self.reduce_words_final)]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MRWordFreqCountSort.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "Creating temp directory /var/folders/rs/x38wjyw962d7804bsghvbxrn37md3k/T/MRWordFreqCountSort.z013hnv.20160707.212422.141760\n",
      "Running step 1 of 2...\n",
      "Running step 2 of 2...\n",
      "***Top 50***\n",
      "('yes', 369651)\n",
      "('closed', 309303)\n",
      "('with', 288613)\n",
      "('mortgage', 267787)\n",
      "('2014', 260871)\n",
      "('no', 229714)\n",
      "('explanation', 215782)\n",
      "('2013', 215714)\n",
      "('web', 183716)\n",
      "('loan', 170400)\n",
      "('credit', 159787)\n",
      "('2012', 144318)\n",
      "('account', 143647)\n",
      "('collection', 118073)\n",
      "('bank', 108378)\n",
      "('debt', 98527)\n",
      "('or', 95366)\n",
      "('relief', 86076)\n",
      "('07', 80935)\n",
      "('other', 80052)\n",
      "('08', 79720)\n",
      "('06', 79413)\n",
      "('10', 78830)\n",
      "('03', 77247)\n",
      "('referral', 77087)\n",
      "('05', 76342)\n",
      "('04', 75741)\n",
      "('09', 75317)\n",
      "('of', 71382)\n",
      "('modification', 70487)\n",
      "('foreclosure', 70487)\n",
      "('02', 68426)\n",
      "('01', 67421)\n",
      "('monetary', 62838)\n",
      "('12', 56896)\n",
      "('card', 55750)\n",
      "('11', 55201)\n",
      "('conventional', 50516)\n",
      "('non', 50093)\n",
      "('not', 48628)\n",
      "('reporting', 47793)\n",
      "('ca', 47076)\n",
      "('servicing', 45986)\n",
      "('service', 44950)\n",
      "('america', 42530)\n",
      "('report', 40675)\n",
      "('payments', 40000)\n",
      "('information', 39523)\n",
      "('phone', 37790)\n",
      "('fixed', 36857)\n",
      "***Bottom 10***\n",
      "('00001', 1)\n",
      "('00002', 1)\n",
      "('00005', 1)\n",
      "('00008', 1)\n",
      "('00017', 1)\n",
      "('00018', 1)\n",
      "('00023', 1)\n",
      "('00024', 1)\n",
      "('00026', 1)\n",
      "('00032', 1)\n",
      "Streaming final output from /var/folders/rs/x38wjyw962d7804bsghvbxrn37md3k/T/MRWordFreqCountSort.z013hnv.20160707.212422.141760/output...\n",
      "Removing temp directory /var/folders/rs/x38wjyw962d7804bsghvbxrn37md3k/T/MRWordFreqCountSort.z013hnv.20160707.212422.141760...\n"
     ]
    }
   ],
   "source": [
    "!python MRWordFreqCountSort.py --jobconf mapred.reduce.tasks=1 Consumer_Complaints.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW2.2.2:   \n",
    "Repeat HW2.2.1 using 3 reducers. Use the same code as in HW2.2.1  with just one modification \n",
    "to the command line: just add --jobconf mapred.reduce.tasks=3 as see presented here: \n",
    "\n",
    "    python HW2.2WordCount.py --jobconf mapred.reduce.tasks=3 oneLinerTextFile.txt\n",
    "\n",
    "Describe what you see. Is this correct?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "Creating temp directory /var/folders/rs/x38wjyw962d7804bsghvbxrn37md3k/T/MRWordFreqCountSort.z013hnv.20160707.212659.962669\n",
      "Running step 1 of 2...\n",
      "Running step 2 of 2...\n",
      "***Top 50***\n",
      "('375865', 1)\n",
      "('375861', 1)\n",
      "('37586', 1)\n",
      "('375859', 1)\n",
      "('375856', 1)\n",
      "('375855', 1)\n",
      "('375852', 1)\n",
      "('375851', 1)\n",
      "('37585', 1)\n",
      "('375847', 1)\n",
      "('375845', 1)\n",
      "('375843', 1)\n",
      "('375842', 1)\n",
      "('375841', 1)\n",
      "('375840', 1)\n",
      "('375839', 1)\n",
      "('375838', 1)\n",
      "('375837', 1)\n",
      "('375835', 1)\n",
      "('375834', 1)\n",
      "('375833', 1)\n",
      "('375831', 1)\n",
      "('375829', 1)\n",
      "('375826', 1)\n",
      "('375823', 1)\n",
      "('375820', 1)\n",
      "('375819', 1)\n",
      "('375817', 1)\n",
      "('375816', 1)\n",
      "('375815', 1)\n",
      "('375814', 1)\n",
      "('375813', 1)\n",
      "('375812', 1)\n",
      "('375810', 1)\n",
      "('375805', 1)\n",
      "('375804', 1)\n",
      "('37580', 1)\n",
      "('375798', 1)\n",
      "('375796', 1)\n",
      "('375795', 1)\n",
      "('375794', 1)\n",
      "('375791', 1)\n",
      "('375788', 1)\n",
      "('375786', 1)\n",
      "('375784', 1)\n",
      "('375780', 1)\n",
      "('375777', 1)\n",
      "('375776', 1)\n",
      "('375775', 1)\n",
      "('375770', 1)\n",
      "***Bottom 10***\n",
      "('00001', 1)\n",
      "('00002', 1)\n",
      "('00005', 1)\n",
      "('00008', 1)\n",
      "('00017', 1)\n",
      "('00018', 1)\n",
      "('00023', 1)\n",
      "('00024', 1)\n",
      "('00026', 1)\n",
      "('00032', 1)\n",
      "***Top 50***\n",
      "('724050', 1)\n",
      "('724047', 1)\n",
      "('724039', 1)\n",
      "('724034', 1)\n",
      "('724032', 1)\n",
      "('724030', 1)\n",
      "('72403', 1)\n",
      "('724024', 1)\n",
      "('724023', 1)\n",
      "('724022', 1)\n",
      "('724018', 1)\n",
      "('724011', 1)\n",
      "('724008', 1)\n",
      "('724007', 1)\n",
      "('723997', 1)\n",
      "('723995', 1)\n",
      "('723990', 1)\n",
      "('723989', 1)\n",
      "('723987', 1)\n",
      "('723978', 1)\n",
      "('723969', 1)\n",
      "('723967', 1)\n",
      "('723966', 1)\n",
      "('723965', 1)\n",
      "('723963', 1)\n",
      "('723958', 1)\n",
      "('723956', 1)\n",
      "('723950', 1)\n",
      "('723947', 1)\n",
      "('723942', 1)\n",
      "('723941', 1)\n",
      "('723930', 1)\n",
      "('72393', 1)\n",
      "('723924', 1)\n",
      "('723920', 1)\n",
      "('723913', 1)\n",
      "('723910', 1)\n",
      "('723905', 1)\n",
      "('723901', 1)\n",
      "('723900', 1)\n",
      "('723895', 1)\n",
      "('723891', 1)\n",
      "('723889', 1)\n",
      "('723886', 1)\n",
      "('723880', 1)\n",
      "('723878', 1)\n",
      "('723874', 1)\n",
      "('723870', 1)\n",
      "('72387', 1)\n",
      "('723869', 1)\n",
      "***Bottom 10***\n",
      "('375866', 1)\n",
      "('375869', 1)\n",
      "('375871', 1)\n",
      "('375874', 1)\n",
      "('375876', 1)\n",
      "('375877', 1)\n",
      "('375881', 1)\n",
      "('375882', 1)\n",
      "('375886', 1)\n",
      "('375887', 1)\n",
      "***Top 50***\n",
      "('yes', 369651)\n",
      "('closed', 309303)\n",
      "('with', 288613)\n",
      "('mortgage', 267787)\n",
      "('2014', 260871)\n",
      "('no', 229714)\n",
      "('explanation', 215782)\n",
      "('2013', 215714)\n",
      "('web', 183716)\n",
      "('loan', 170400)\n",
      "('credit', 159787)\n",
      "('2012', 144318)\n",
      "('account', 143647)\n",
      "('collection', 118073)\n",
      "('bank', 108378)\n",
      "('debt', 98527)\n",
      "('or', 95366)\n",
      "('relief', 86076)\n",
      "('07', 80935)\n",
      "('other', 80052)\n",
      "('08', 79720)\n",
      "('06', 79413)\n",
      "('10', 78830)\n",
      "('03', 77247)\n",
      "('referral', 77087)\n",
      "('05', 76342)\n",
      "('04', 75741)\n",
      "('09', 75317)\n",
      "('of', 71382)\n",
      "('modification', 70487)\n",
      "('foreclosure', 70487)\n",
      "('02', 68426)\n",
      "('01', 67421)\n",
      "('monetary', 62838)\n",
      "('12', 56896)\n",
      "('card', 55750)\n",
      "('11', 55201)\n",
      "('conventional', 50516)\n",
      "('non', 50093)\n",
      "('not', 48628)\n",
      "('reporting', 47793)\n",
      "('ca', 47076)\n",
      "('servicing', 45986)\n",
      "('service', 44950)\n",
      "('america', 42530)\n",
      "('report', 40675)\n",
      "('payments', 40000)\n",
      "('information', 39523)\n",
      "('phone', 37790)\n",
      "('fixed', 36857)\n",
      "***Bottom 10***\n",
      "('724052', 1)\n",
      "('724053', 1)\n",
      "('724054', 1)\n",
      "('724063', 1)\n",
      "('724065', 1)\n",
      "('724072', 1)\n",
      "('724076', 1)\n",
      "('724077', 1)\n",
      "('724081', 1)\n",
      "('724083', 1)\n",
      "Streaming final output from /var/folders/rs/x38wjyw962d7804bsghvbxrn37md3k/T/MRWordFreqCountSort.z013hnv.20160707.212659.962669/output...\n",
      "Removing temp directory /var/folders/rs/x38wjyw962d7804bsghvbxrn37md3k/T/MRWordFreqCountSort.z013hnv.20160707.212659.962669...\n"
     ]
    }
   ],
   "source": [
    "!python MRWordFreqCountSort.py --jobconf mapred.reduce.tasks=3 Consumer_Complaints.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HW2.2.2 Using 3 reducers for this sort task ends up sorting and printing the list in 3 different chunks.  Each list is correct on it's own, but the point of the exercise was to sort and print the entire list - which means this is incorrect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW2.2.3: [Optional; we will cover this in class]  \n",
    "Solve the \"total sort\" issue in HW2.2.2. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HW2.2.3 Looking forward to it :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW2.3: Shopping Cart Analysis\n",
    "Product Recommendations: The action or practice of selling additional products or services \n",
    "to existing customers is called cross-selling. Giving product recommendation is \n",
    "one of the examples of cross-selling that are frequently used by online retailers. \n",
    "One simple method to give product recommendations is to recommend products that are frequently\n",
    "browsed together by the customers.\n",
    "  \n",
    "For this homework use the online browsing behavior dataset located at: \n",
    "\n",
    "       https://www.dropbox.com/s/zlfyiwa70poqg74/ProductPurchaseData.txt?dl=0\n",
    "\n",
    "Each line in this dataset represents a browsing session of a customer. \n",
    "On each line, each string of 8 characters represents the id of an item browsed during that session. \n",
    "The items are separated by spaces.\n",
    "\n",
    "Here are the first few lines of the ProductPurchaseData \n",
    "FRO11987 ELE17451 ELE89019 SNA90258 GRO99222 \n",
    "GRO99222 GRO12298 FRO12685 ELE91550 SNA11465 ELE26917 ELE52966 FRO90334 SNA30755 ELE17451 FRO84225 SNA80192 \n",
    "ELE17451 GRO73461 DAI22896 SNA99873 FRO86643 \n",
    "ELE17451 ELE37798 FRO86643 GRO56989 ELE23393 SNA11465 \n",
    "ELE17451 SNA69641 FRO86643 FRO78087 SNA11465 GRO39357 ELE28573 ELE11375 DAI54444 \n",
    "\n",
    "\n",
    "Do some exploratory data analysis of this dataset guided by the following questions:. \n",
    "\n",
    "How many unique items are available from this supplier?\n",
    "\n",
    "Using a single reducer: Report your findings: such as number of unique products; largest basket; report the top 50 most frequently purchased items,  their frequency,   (break ties by sorting the products alphabetical order) etc. using Hadoop Map-Reduce. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting wordcountSCA.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile wordcountSCA.py\n",
    "#HW2.3\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import sys, re, string, operator\n",
    "\n",
    "class wordCountSCA(MRJob):\n",
    "    \n",
    "    def get_words(self, _, line):\n",
    "        regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "        line = regex.sub(' ', str(line).lower())\n",
    "        line = re.sub( '\\s+', ' ', line )\n",
    "        \n",
    "        words = line.split()                                          # split each basket into items\n",
    "        basket = ''.join(map(str, words))                             # create a unique key for each basket\n",
    "\n",
    "        for w in words: \n",
    "            yield(w, 1)                                               # count each item once for item totals\n",
    "            yield(\"BASKET \"+basket, 1)                                # count each item once for basket size\n",
    "    \n",
    "    def sum_words(self, word, values):\n",
    "        yield(word, sum(values))\n",
    "        \n",
    "    def sort_words(self, word, value):\n",
    "        newkey = '{number:012d}'.format(number=int(value))+word\n",
    "        yield(newkey, [word, value])\n",
    "\n",
    "    def red_init(self):\n",
    "        self.d=[]\n",
    "        self.bs=[]\n",
    "    \n",
    "    def reduce_words(self, key, values):\n",
    "        for value in values:\n",
    "            if value[0][:6]!=\"BASKET\":\n",
    "                self.d.append([value[0], value[1]])\n",
    "            else:\n",
    "                self.bs.append([value[0], value[1]])\n",
    "            \n",
    "    def reduce_words_final(self):\n",
    "        unique_items = len(self.d)\n",
    "        num_baskets = len(self.bs)\n",
    "        max_d = unique_items-1\n",
    "        max_bs = num_baskets-1\n",
    "        print(\"Number of unique items is %s\" % unique_items)\n",
    "        print(\"Largest basket has %s items\" % self.bs[max_bs][1])\n",
    "        print(\"***Top 50***\")\n",
    "        for i in range(50):\n",
    "            print(self.d[max_d - i][0], self.d[max_d - i][1]) \n",
    "        \n",
    "    def steps(self):\n",
    "        return [MRStep(mapper=self.get_words,\n",
    "                      reducer=self.sum_words),\n",
    "                MRStep(mapper=self.sort_words,\n",
    "                      reducer_init=self.red_init,\n",
    "                      reducer=self.reduce_words,\n",
    "                      reducer_final=self.reduce_words_final)]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    wordCountSCA.run()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "Creating temp directory /var/folders/rs/x38wjyw962d7804bsghvbxrn37md3k/T/wordCountSCA.z013hnv.20160708.011531.218113\n",
      "Running step 1 of 2...\n",
      "Running step 2 of 2...\n",
      "Number of unique items is 12592\n",
      "Largest basket has 288 items\n",
      "***Top 50***\n",
      "('dai62779', 6667)\n",
      "('fro40251', 3881)\n",
      "('ele17451', 3875)\n",
      "('gro73461', 3602)\n",
      "('sna80324', 3044)\n",
      "('ele32164', 2851)\n",
      "('dai75645', 2736)\n",
      "('sna45677', 2455)\n",
      "('fro31317', 2330)\n",
      "('dai85309', 2293)\n",
      "('ele26917', 2292)\n",
      "('fro80039', 2233)\n",
      "('gro21487', 2115)\n",
      "('sna99873', 2083)\n",
      "('gro59710', 2004)\n",
      "('gro71621', 1920)\n",
      "('fro85978', 1918)\n",
      "('gro30386', 1840)\n",
      "('ele74009', 1816)\n",
      "('gro56726', 1784)\n",
      "('dai63921', 1773)\n",
      "('gro46854', 1756)\n",
      "('ele66600', 1713)\n",
      "('dai83733', 1712)\n",
      "('fro32293', 1702)\n",
      "('ele66810', 1697)\n",
      "('sna55762', 1646)\n",
      "('dai22177', 1627)\n",
      "('fro78087', 1531)\n",
      "('ele99737', 1516)\n",
      "('gro94758', 1489)\n",
      "('ele34057', 1489)\n",
      "('fro35904', 1436)\n",
      "('fro53271', 1420)\n",
      "('sna93860', 1407)\n",
      "('sna90094', 1390)\n",
      "('gro38814', 1352)\n",
      "('ele56788', 1345)\n",
      "('gro61133', 1321)\n",
      "('ele74482', 1316)\n",
      "('dai88807', 1316)\n",
      "('ele59935', 1311)\n",
      "('sna96271', 1295)\n",
      "('dai43223', 1290)\n",
      "('ele91337', 1289)\n",
      "('gro15017', 1275)\n",
      "('dai31081', 1261)\n",
      "('gro81087', 1220)\n",
      "('dai22896', 1219)\n",
      "('gro85051', 1214)\n",
      "Streaming final output from /var/folders/rs/x38wjyw962d7804bsghvbxrn37md3k/T/wordCountSCA.z013hnv.20160708.011531.218113/output...\n",
      "Removing temp directory /var/folders/rs/x38wjyw962d7804bsghvbxrn37md3k/T/wordCountSCA.z013hnv.20160708.011531.218113...\n"
     ]
    }
   ],
   "source": [
    "!python wordCountSCA.py --jobconf mapred.reduce.tasks=1 ProductPurchaseData.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.3.1 OPTIONAL \n",
    "Using 2 reducers:  Report your findings such as number of unique products; largest basket; report the top 50 most frequently purchased items,  their frequency,  and their frequency (break ties by sorting the products alphabetical order) etc. using Hadoop Map-Reduce. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "Creating temp directory /var/folders/rs/x38wjyw962d7804bsghvbxrn37md3k/T/wordCountSCA.z013hnv.20160708.011739.135101\n",
      "Running step 1 of 2...\n",
      "Running step 2 of 2...\n",
      "Number of unique items is 9646\n",
      "Largest basket has 13 items\n",
      "***Top 50***\n",
      "('sna98720', 12)\n",
      "('sna85641', 12)\n",
      "('sna84439', 12)\n",
      "('sna83731', 12)\n",
      "('sna82197', 12)\n",
      "('sna81989', 12)\n",
      "('sna77281', 12)\n",
      "('sna72305', 12)\n",
      "('sna71126', 12)\n",
      "('sna61679', 12)\n",
      "('sna58201', 12)\n",
      "('sna54318', 12)\n",
      "('sna52223', 12)\n",
      "('sna46099', 12)\n",
      "('sna37804', 12)\n",
      "('sna34587', 12)\n",
      "('sna31046', 12)\n",
      "('sna29452', 12)\n",
      "('sna28368', 12)\n",
      "('sna26649', 12)\n",
      "('sna23770', 12)\n",
      "('sna20107', 12)\n",
      "('gro91087', 12)\n",
      "('gro88061', 12)\n",
      "('gro85444', 12)\n",
      "('gro84449', 12)\n",
      "('gro84258', 12)\n",
      "('gro83392', 12)\n",
      "('gro81597', 12)\n",
      "('gro80995', 12)\n",
      "('gro75023', 12)\n",
      "('gro68313', 12)\n",
      "('gro66865', 12)\n",
      "('gro62704', 12)\n",
      "('gro59094', 12)\n",
      "('gro52534', 12)\n",
      "('gro50736', 12)\n",
      "('gro49643', 12)\n",
      "('gro49322', 12)\n",
      "('gro46512', 12)\n",
      "('gro43461', 12)\n",
      "('gro39193', 12)\n",
      "('gro37355', 12)\n",
      "('gro34212', 12)\n",
      "('gro34170', 12)\n",
      "('gro29410', 12)\n",
      "('gro29257', 12)\n",
      "('gro27151', 12)\n",
      "('gro23893', 12)\n",
      "('gro19142', 12)\n",
      "Number of unique items is 2946\n",
      "Largest basket has 288 items\n",
      "***Top 50***\n",
      "('dai62779', 6667)\n",
      "('fro40251', 3881)\n",
      "('ele17451', 3875)\n",
      "('gro73461', 3602)\n",
      "('sna80324', 3044)\n",
      "('ele32164', 2851)\n",
      "('dai75645', 2736)\n",
      "('sna45677', 2455)\n",
      "('fro31317', 2330)\n",
      "('dai85309', 2293)\n",
      "('ele26917', 2292)\n",
      "('fro80039', 2233)\n",
      "('gro21487', 2115)\n",
      "('sna99873', 2083)\n",
      "('gro59710', 2004)\n",
      "('gro71621', 1920)\n",
      "('fro85978', 1918)\n",
      "('gro30386', 1840)\n",
      "('ele74009', 1816)\n",
      "('gro56726', 1784)\n",
      "('dai63921', 1773)\n",
      "('gro46854', 1756)\n",
      "('ele66600', 1713)\n",
      "('dai83733', 1712)\n",
      "('fro32293', 1702)\n",
      "('ele66810', 1697)\n",
      "('sna55762', 1646)\n",
      "('dai22177', 1627)\n",
      "('fro78087', 1531)\n",
      "('ele99737', 1516)\n",
      "('gro94758', 1489)\n",
      "('ele34057', 1489)\n",
      "('fro35904', 1436)\n",
      "('fro53271', 1420)\n",
      "('sna93860', 1407)\n",
      "('sna90094', 1390)\n",
      "('gro38814', 1352)\n",
      "('ele56788', 1345)\n",
      "('gro61133', 1321)\n",
      "('ele74482', 1316)\n",
      "('dai88807', 1316)\n",
      "('ele59935', 1311)\n",
      "('sna96271', 1295)\n",
      "('dai43223', 1290)\n",
      "('ele91337', 1289)\n",
      "('gro15017', 1275)\n",
      "('dai31081', 1261)\n",
      "('gro81087', 1220)\n",
      "('dai22896', 1219)\n",
      "('gro85051', 1214)\n",
      "Streaming final output from /var/folders/rs/x38wjyw962d7804bsghvbxrn37md3k/T/wordCountSCA.z013hnv.20160708.011739.135101/output...\n",
      "Removing temp directory /var/folders/rs/x38wjyw962d7804bsghvbxrn37md3k/T/wordCountSCA.z013hnv.20160708.011739.135101...\n"
     ]
    }
   ],
   "source": [
    "!python wordCountSCA.py --jobconf mapred.reduce.tasks=2 ProductPurchaseData.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW2.4. (Computationally prohibitive but then again Hadoop can handle this) Pairs\n",
    "\n",
    "From a data mining perspective (and the aPriori algorihtm in particular), Support and Confidence are defined as follows:\n",
    "\n",
    "       SUPPORT\n",
    "       In data mining, the support value of X (where X is a collection of cooccurring items sometimes referred to as \n",
    "       an item-set. E.g., a basket or subset of a basket) with respect to T  (a transaction database where each \n",
    "       row is a transaction such as a basket of items that have been purchased)  is defined as the proportion \n",
    "       of transactions in the  database which contains  the item-set X. (a relative frequency of sorts) \n",
    "\n",
    "       CONFIDENCE \n",
    "       The confidence value of a rule, X ==>  Y (where X is a collection of cooccurring items and Y is generally \n",
    "       a single item. E.g., If Diapers and Beer then Cigars were also purchased), with respect to a set of transactions T, is the \n",
    "       proportion of the transactions that contains X which also contains Y. (Think of it as  tgePr(Y|X) )\n",
    "\n",
    "       The pairs/stripes algorithm returns cooccurrence information that can be used directly to  calculate the confidence and support. \n",
    "       Note that confidence for pair X ==>  Y will  differ from the relative frequency that results from stripes when X occurs by itself in transactions.\n",
    "\n",
    "\n",
    "Suppose we want to recommend new products to the customer based on the products they\n",
    "have already browsed on the online website. Write a map-reduce program \n",
    "to find products which are frequently browsed together. Fix the support count (cooccurence count) to s = 100 \n",
    "(i.e. product pairs need to occur together at least 100 times to be considered frequent) \n",
    "and find pairs of items (sometimes referred to itemsets of size 2 in association rule mining) that have a support count of 100 or more.\n",
    "\n",
    "List the top 50 product pairs with corresponding support count (aka frequency), and relative frequency or support (number of records where they coccur, the number of records where they coccur/the number of baskets in the dataset)  in decreasing order of support  for frequent (100>count) itemsets of size 2. \n",
    "\n",
    "Use the Pairs pattern  to  extract these frequent itemsets of size 2. Free free to use combiners if they bring value. Instrument your code with counters for count the number of times your mapper, combiner and reducers are called.  \n",
    "\n",
    "Please output records of the following form for the top 50 pairs (itemsets of size 2): \n",
    "\n",
    "      item1, item2, support count, support  (OPTIONAL Feel free to add in confidence level also)\n",
    "\n",
    "\n",
    "\n",
    "Fix the ordering of the pairs lexicographically (left to right), \n",
    "and break ties in support (between pairs, if any exist) \n",
    "by taking the first ones in lexicographically increasing order. \n",
    "\n",
    "Report  the compute time for the Pairs job. Describe the computational setup used (E.g., single computer; dual core; linux, number of mappers, number of reducers)\n",
    "Instrument your mapper, combiner, and reducer to count how many times each is called using Counters and report these counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting pairsSCA.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile pairsSCA.py\n",
    "#HW2.4\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import sys, re, string, operator, itertools\n",
    "\n",
    "class pairsSCA(MRJob):                  \n",
    "    \n",
    "    def get_words(self, _, line):\n",
    "        self.increment_counter('group', 'Num_mapper_step1_calls', 1)\n",
    "        regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "        line = regex.sub(' ', str(line).lower())\n",
    "        line = re.sub( '\\s+', ' ', line )\n",
    "        \n",
    "        words = line.split()                                          # split each basket into items\n",
    "        basket = ''.join(map(str, words))                             # create a unique key for each basket\n",
    "\n",
    "        for w in words: \n",
    "            yield(w, 1)                                               # count each item once for item totals\n",
    "            yield(\"BASKET \"+basket, 1)                                # count each item once for basket size\n",
    "        for subset in itertools.combinations(sorted(set(words)), 2):\n",
    "            yield(\"PAIRS \"+str(subset[0])+\" - \"+str(subset[1]), 1)    # count each pair once\n",
    "\n",
    "    def sum_words(self, word, values):\n",
    "        self.increment_counter('group', 'Num_reducer_step1_calls', 1)\n",
    "        yield(word, sum(values))\n",
    "        \n",
    "    def sort_words(self, word, value):\n",
    "        self.increment_counter('group', 'Num_mapper_step2_calls', 1)\n",
    "        newkey = '{number:012d}'.format(number=int(value))+word\n",
    "        yield(newkey, [word, value])\n",
    "\n",
    "    def red_init(self):\n",
    "        self.d=[]\n",
    "        self.bs=[]\n",
    "        self.pairs=[]\n",
    "        self.freqpairs=[]\n",
    "    \n",
    "    def reduce_words(self, key, values):\n",
    "        self.increment_counter('group', 'Num_reducer_step2_calls', 1)\n",
    "        for value in values:\n",
    "            if value[0][:6]==\"BASKET\":\n",
    "                self.bs.append([value[0], value[1]])\n",
    "            elif value[0][:5]==\"PAIRS\":\n",
    "                self.pairs.append([value[0][6:], value[1]])\n",
    "            else:\n",
    "                self.d.append([value[0], value[1]])\n",
    "            \n",
    "    def reduce_words_final(self):\n",
    "        unique_items = len(self.d)\n",
    "        num_baskets = len(self.bs)\n",
    "        num_pairs = len(self.pairs)\n",
    "        max_d = unique_items-1\n",
    "        max_bs = num_baskets-1\n",
    "        max_pairs = num_pairs-1\n",
    "        print(\"Number of unique items is %s\" % unique_items)\n",
    "        print(\"Number of baskets is %s\" % num_baskets)\n",
    "        print(\"Largest basket has %s items\" % self.bs[max_bs][1])\n",
    "        print(\"Number of total pairs is %s\" % num_pairs)\n",
    "        for k in range(num_pairs):\n",
    "            if self.pairs[k][1]>100:\n",
    "                self.freqpairs.append([self.pairs[0], self.pairs[1]])\n",
    "        num_freqpairs = len(self.freqpairs)\n",
    "        print(\"Number of frequent pairs is %s\" % num_freqpairs)\n",
    "        print(\"***Top 50 pairs***\")\n",
    "        for i in range(50):\n",
    "            print(self.pairs[max_pairs - i][0][:8], self.pairs[max_pairs - i][0][11:],\n",
    "                  self.pairs[max_pairs - i][1], float(self.pairs[max_pairs - i][1])/num_baskets)\n",
    "        \n",
    "            \n",
    "    def steps(self):\n",
    "        return [MRStep(mapper=self.get_words,\n",
    "                      reducer=self.sum_words),\n",
    "                MRStep(mapper=self.sort_words,\n",
    "                      reducer_init=self.red_init,\n",
    "                      reducer=self.reduce_words,\n",
    "                      reducer_final=self.reduce_words_final)]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    pairsSCA.run()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "Creating temp directory /var/folders/rs/x38wjyw962d7804bsghvbxrn37md3k/T/pairsSCA.z013hnv.20160708.033048.144792\n",
      "Running step 1 of 2...\n",
      "Counters: 1\n",
      "\tgroup\n",
      "\t\tNum_mapper_step1_calls=31101\n",
      "Counters: 2\n",
      "\tgroup\n",
      "\t\tNum_mapper_step1_calls=31101\n",
      "\t\tNum_reducer_step1_calls=920384\n",
      "Running step 2 of 2...\n",
      "Counters: 1\n",
      "\tgroup\n",
      "\t\tNum_mapper_step2_calls=920384\n",
      "Number of unique items is 12592\n",
      "Number of baskets is 30697\n",
      "Largest basket has 288 items\n",
      "Number of total pairs is 877095\n",
      "Number of frequent pairs is 1311\n",
      "***Top 50 pairs***\n",
      "('dai62779', 'ele17451', 1592, 0.05186174544743786)\n",
      "('fro40251', 'sna80324', 1412, 0.0459979802586572)\n",
      "('dai75645', 'fro40251', 1254, 0.040850897481838615)\n",
      "('fro40251', 'gro85051', 1213, 0.039515262077727466)\n",
      "('dai62779', 'gro73461', 1139, 0.037104603055673195)\n",
      "('dai75645', 'sna80324', 1130, 0.03681141479623416)\n",
      "('dai62779', 'fro40251', 1070, 0.034856826399973936)\n",
      "('dai62779', 'sna80324', 923, 0.030068084829136397)\n",
      "('dai62779', 'dai85309', 918, 0.029905202462781378)\n",
      "('ele32164', 'gro59710', 911, 0.029677167149884352)\n",
      "('fro40251', 'gro73461', 882, 0.028732449425025248)\n",
      "('dai62779', 'dai75645', 882, 0.028732449425025248)\n",
      "('dai62779', 'ele92920', 877, 0.02856956705867023)\n",
      "('fro40251', 'fro92469', 835, 0.027201355181288075)\n",
      "('dai62779', 'ele32164', 832, 0.027103625761475063)\n",
      "('dai75645', 'gro73461', 712, 0.02319444896895462)\n",
      "('dai43223', 'ele32164', 711, 0.023161872495683616)\n",
      "('dai62779', 'gro30386', 709, 0.02309671954914161)\n",
      "('ele17451', 'fro40251', 697, 0.022705801869889564)\n",
      "('dai85309', 'ele99737', 659, 0.021467895885591427)\n",
      "('dai62779', 'ele26917', 650, 0.02117470762615239)\n",
      "('gro21487', 'gro73461', 631, 0.020555754634003325)\n",
      "('dai62779', 'sna45677', 604, 0.019676189855686223)\n",
      "('ele17451', 'sna80324', 597, 0.019448154542789198)\n",
      "('dai62779', 'gro71621', 595, 0.01938300159624719)\n",
      "('dai62779', 'sna55762', 593, 0.019317848649705184)\n",
      "('dai62779', 'dai83733', 586, 0.01908981333680816)\n",
      "('ele17451', 'gro73461', 580, 0.018894354497182134)\n",
      "('gro73461', 'sna80324', 562, 0.01830797797830407)\n",
      "('dai62779', 'gro59710', 561, 0.018275401505033064)\n",
      "('dai62779', 'fro80039', 550, 0.017917060299052025)\n",
      "('dai75645', 'ele17451', 547, 0.017819330879239013)\n",
      "('dai62779', 'sna93860', 537, 0.017493566146528975)\n",
      "('dai55148', 'dai62779', 526, 0.017135224940547936)\n",
      "('dai43223', 'gro59710', 512, 0.016679154314753884)\n",
      "('ele17451', 'ele32164', 511, 0.016646577841482883)\n",
      "('dai62779', 'sna18336', 506, 0.016483695475127864)\n",
      "('ele32164', 'gro73461', 486, 0.01583216600970779)\n",
      "('dai85309', 'ele17451', 482, 0.015701860116623775)\n",
      "('dai62779', 'fro78087', 482, 0.015701860116623775)\n",
      "('dai62779', 'gro94758', 479, 0.015604130696810763)\n",
      "('gro85051', 'sna80324', 471, 0.015343518910642734)\n",
      "('dai62779', 'gro21487', 471, 0.015343518910642734)\n",
      "('ele17451', 'gro30386', 468, 0.015245789490829723)\n",
      "('fro85978', 'sna95666', 463, 0.015082907124474704)\n",
      "('dai62779', 'fro19221', 462, 0.015050330651203701)\n",
      "('dai62779', 'gro46854', 461, 0.015017754177932698)\n",
      "('dai43223', 'dai62779', 459, 0.014952601231390689)\n",
      "('ele92920', 'sna18336', 455, 0.014822295338306675)\n",
      "('dai88079', 'fro40251', 446, 0.014529107078867641)\n",
      "Counters: 2\n",
      "\tgroup\n",
      "\t\tNum_mapper_step2_calls=920384\n",
      "\t\tNum_reducer_step2_calls=920384\n",
      "Streaming final output from /var/folders/rs/x38wjyw962d7804bsghvbxrn37md3k/T/pairsSCA.z013hnv.20160708.033048.144792/output...\n",
      "Removing temp directory /var/folders/rs/x38wjyw962d7804bsghvbxrn37md3k/T/pairsSCA.z013hnv.20160708.033048.144792...\n",
      "CPU times: user 692 ms, sys: 293 ms, total: 985 ms\n",
      "Wall time: 1min 37s\n"
     ]
    }
   ],
   "source": [
    "%time !python pairsSCA.py --jobconf mapred.reduce.tasks=1 ProductPurchaseData.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW2.5: Stripes\n",
    "Repeat 2.4 using the stripes design pattern for finding cooccuring pairs (and out.\n",
    "\n",
    "Report  the compute times for stripes job versus the Pairs job. Describe the computational setup used (E.g., single computer; dual core; linux, number of mappers, number of reducers)\n",
    "\n",
    "Instrument your mapper, combiner, and reducer to count how many times each is called using Counters and report these counts. Discuss the differences in these counts between the Pairs and Stripes jobs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting stripes_madSCA.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile stripes_madSCA.py\n",
    "#HW2.5\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import sys, re, string, operator, itertools\n",
    "\n",
    "class stripes_madSCA(MRJob):                  \n",
    "    \n",
    "    def get_words(self, _, line):\n",
    "        self.increment_counter('group', 'Num_mapper_calls', 1)\n",
    "        regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "        line = regex.sub(' ', str(line).lower())\n",
    "        line = re.sub( '\\s+', ' ', line )\n",
    "        \n",
    "        words = line.split()                                          # split each basket into items\n",
    "        basket = ''.join(map(str, words))                             # create a unique key for each basket\n",
    "\n",
    "        for w in words: \n",
    "            #yield(w, 1)                                               # count each item once for item totals\n",
    "            yield(\"BASKET \"+basket, 1)                                # count each item once for basket size\n",
    "        getdic={ word:{ subset[1]:1 for subset in itertools.combinations(sorted(set(words)), 2) if subset[0]==word} for word in words }\n",
    "        for k, v in getdic.iteritems():\n",
    "            if v:\n",
    "                yield(\"STRIPE \", [k, v])\n",
    "        \n",
    "    def init_combine_stripes(self):\n",
    "        self.dic = {}\n",
    "     \n",
    "    def simple_comb(self, key, values):\n",
    "        self.increment_counter('group', 'Num_combiner_calls', 1)\n",
    "        if key == \"STRIPE \":\n",
    "            for k, v in values:\n",
    "                yield(\"STRIPE \", [k, v])\n",
    "        else:\n",
    "            yield(\"BASKET \", [key, sum(values)])\n",
    "\n",
    "    def init_reduce_stripes(self):\n",
    "        self.reddic = {}\n",
    "        self.bs = {}\n",
    "        self.pairs = []\n",
    "\n",
    "    def simple_red(self, key, values):\n",
    "        self.increment_counter('group', 'Num_reducer_calls', 1)\n",
    "        if key == \"STRIPE \":\n",
    "            for k, v in values:\n",
    "                if self.reddic.get(k):\n",
    "                    x = self.reddic[k]\n",
    "                    y = v\n",
    "                    self.reddic[k] = { n: x.get(n, 0) + y.get(n, 0) for n in set(x) | set(y) }\n",
    "                else:\n",
    "                    self.reddic[k]=v\n",
    "        else:\n",
    "            for value in values:\n",
    "                yield(value[0], value[1])\n",
    "        \n",
    "        for k, v in self.reddic.iteritems():\n",
    "            for i in v:\n",
    "                yield(k +' - '+ i, v[i])\n",
    "            \n",
    "    def sort_words(self, word, value):\n",
    "        self.increment_counter('group', 'Num_mapper_step2_calls', 1)\n",
    "        newkey = '{number:012d}'.format(number=int(value))+word\n",
    "        yield(newkey, [word, value])\n",
    "\n",
    "    def red_init(self):\n",
    "        self.bs=[]\n",
    "        self.pairs=[]\n",
    "        self.freqpairs=[]\n",
    "    \n",
    "    def reduce_words(self, key, values):\n",
    "        self.increment_counter('group', 'Num_reducer_step2_calls', 1)\n",
    "        for value in values:\n",
    "            if value[0][:6]==\"BASKET\":\n",
    "                self.bs.append([value[0], value[1]])\n",
    "            else:\n",
    "                self.pairs.append([value[0], value[1]])\n",
    "\n",
    "    def just_print(self):\n",
    "        print self.pairs\n",
    "    \n",
    "    def reduce_words_final(self):\n",
    "        #unique_items = len(self.d)\n",
    "        num_baskets = len(self.bs)\n",
    "        num_pairs = len(self.pairs)\n",
    "        #max_d = unique_items-1\n",
    "        max_bs = num_baskets-1\n",
    "        max_pairs = num_pairs-1\n",
    "        #print(\"Number of unique items is %s\" % unique_items)\n",
    "        print(\"Number of baskets is %s\" % num_baskets)\n",
    "        print(\"Largest basket has %s items\" % self.bs[max_bs][1])\n",
    "        print(\"Number of total pairs is %s\" % num_pairs)\n",
    "        for k in range(num_pairs):\n",
    "            if self.pairs[k][1]>100:\n",
    "                self.freqpairs.append([self.pairs[0], self.pairs[1]])\n",
    "        num_freqpairs = len(self.freqpairs)\n",
    "        print(\"Number of frequent pairs is %s\" % num_freqpairs)\n",
    "        print(\"***Top 50 pairs***\")\n",
    "        for i in range(50):\n",
    "            print(self.pairs[max_pairs - i][0][:8], self.pairs[max_pairs - i][0][11:],\n",
    "                  self.pairs[max_pairs - i][1], float(self.pairs[max_pairs - i][1])/num_baskets)                \n",
    "\n",
    "    def steps(self):\n",
    "        return [MRStep(mapper=self.get_words,\n",
    "                       combiner_init=self.init_combine_stripes,\n",
    "                       combiner=self.simple_comb,\n",
    "                       reducer_init=self.init_reduce_stripes,\n",
    "                       reducer=self.simple_red),\n",
    "                MRStep(mapper=self.sort_words,\n",
    "                       reducer_init=self.red_init,\n",
    "                       reducer=self.reduce_words,\n",
    "                       reducer_final=self.reduce_words_final)]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    stripes_madSCA.run()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "Creating temp directory /var/folders/rs/x38wjyw962d7804bsghvbxrn37md3k/T/stripes_madSCA.z013hnv.20160709.024643.456789\n",
      "Running step 1 of 2...\n",
      "Counters: 2\n",
      "\tgroup\n",
      "\t\tNum_combiner_calls=30739\n",
      "\t\tNum_mapper_calls=31101\n",
      "Counters: 3\n",
      "\tgroup\n",
      "\t\tNum_combiner_calls=30739\n",
      "\t\tNum_mapper_calls=31101\n",
      "\t\tNum_reducer_calls=2\n",
      "Running step 2 of 2...\n",
      "Counters: 1\n",
      "\tgroup\n",
      "\t\tNum_mapper_step2_calls=907832\n",
      "Number of baskets is 30737\n",
      "Largest basket has 288 items\n",
      "Number of total pairs is 877095\n",
      "Number of frequent pairs is 1311\n",
      "***Top 50 pairs***\n",
      "('dai62779', 'ele17451', 1592, 0.05179425448156944)\n",
      "('fro40251', 'sna80324', 1412, 0.04593812018088948)\n",
      "('dai75645', 'fro40251', 1254, 0.0407977356280704)\n",
      "('fro40251', 'gro85051', 1213, 0.039463838370693304)\n",
      "('dai62779', 'gro73461', 1139, 0.037056316491524875)\n",
      "('dai75645', 'sna80324', 1130, 0.03676350977649087)\n",
      "('dai62779', 'fro40251', 1070, 0.03481146500959755)\n",
      "('dai62779', 'sna80324', 923, 0.030028955330708918)\n",
      "('dai62779', 'dai85309', 918, 0.029866284933467806)\n",
      "('ele32164', 'gro59710', 911, 0.029638546377330252)\n",
      "('fro40251', 'gro73461', 882, 0.028695058073331815)\n",
      "('dai62779', 'dai75645', 882, 0.028695058073331815)\n",
      "('dai62779', 'ele92920', 877, 0.028532387676090704)\n",
      "('fro40251', 'fro92469', 835, 0.02716595633926538)\n",
      "('dai62779', 'ele32164', 832, 0.027068354100920716)\n",
      "('dai75645', 'gro73461', 712, 0.023164264567134073)\n",
      "('dai43223', 'ele32164', 711, 0.02313173048768585)\n",
      "('dai62779', 'gro30386', 709, 0.023066662328789407)\n",
      "('ele17451', 'fro40251', 697, 0.02267625337541074)\n",
      "('dai85309', 'ele99737', 659, 0.021439958356378307)\n",
      "('dai62779', 'ele26917', 650, 0.021147151641344307)\n",
      "('gro21487', 'gro73461', 631, 0.02052900413182809)\n",
      "('dai62779', 'sna45677', 604, 0.019650583986726096)\n",
      "('ele17451', 'sna80324', 597, 0.01942284543058854)\n",
      "('dai62779', 'gro71621', 595, 0.0193577772716921)\n",
      "('dai62779', 'sna55762', 593, 0.019292709112795653)\n",
      "('dai62779', 'dai83733', 586, 0.0190649705566581)\n",
      "('ele17451', 'gro73461', 580, 0.018869766079968767)\n",
      "('gro73461', 'sna80324', 562, 0.01828415264990077)\n",
      "('dai62779', 'gro59710', 561, 0.01825161857045255)\n",
      "('dai62779', 'fro80039', 550, 0.017893743696522108)\n",
      "('dai75645', 'ele17451', 547, 0.017796141458177442)\n",
      "('dai62779', 'sna93860', 537, 0.017470800663695222)\n",
      "('dai55148', 'dai62779', 526, 0.01711292578976478)\n",
      "('dai43223', 'gro59710', 512, 0.01665744867748967)\n",
      "('ele17451', 'ele32164', 511, 0.016624914598041447)\n",
      "('dai62779', 'sna18336', 506, 0.01646224420080034)\n",
      "('ele32164', 'gro73461', 486, 0.0158115626118359)\n",
      "('dai85309', 'ele17451', 482, 0.01568142629404301)\n",
      "('dai62779', 'fro78087', 482, 0.01568142629404301)\n",
      "('dai62779', 'gro94758', 479, 0.015583824055698345)\n",
      "('gro85051', 'sna80324', 471, 0.015323551420112567)\n",
      "('dai62779', 'gro21487', 471, 0.015323551420112567)\n",
      "('ele17451', 'gro30386', 468, 0.015225949181767902)\n",
      "('fro85978', 'sna95666', 463, 0.015063278784526792)\n",
      "('dai62779', 'fro19221', 462, 0.01503074470507857)\n",
      "('dai62779', 'gro46854', 461, 0.014998210625630348)\n",
      "('dai43223', 'dai62779', 459, 0.014933142466733903)\n",
      "('ele92920', 'sna18336', 455, 0.014803006148941016)\n",
      "('dai88079', 'fro40251', 446, 0.014510199433907018)\n",
      "Counters: 2\n",
      "\tgroup\n",
      "\t\tNum_mapper_step2_calls=907832\n",
      "\t\tNum_reducer_step2_calls=907794\n",
      "Streaming final output from /var/folders/rs/x38wjyw962d7804bsghvbxrn37md3k/T/stripes_madSCA.z013hnv.20160709.024643.456789/output...\n",
      "Removing temp directory /var/folders/rs/x38wjyw962d7804bsghvbxrn37md3k/T/stripes_madSCA.z013hnv.20160709.024643.456789...\n",
      "CPU times: user 846 ms, sys: 245 ms, total: 1.09 s\n",
      "Wall time: 2min 21s\n"
     ]
    }
   ],
   "source": [
    "%time !python stripes_madSCA.py --jobconf mapred.reduce.tasks=1 ProductPurchaseData.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# HW2.6: KMeans Clustering Tweet Dataset  [OPTIONAL]\n",
    "\n",
    "For this problem, please refer to and borrow from the following notebook:\n",
    "\n",
    "   http://nbviewer.jupyter.org/urls/dl.dropbox.com/s/dcw8evd9v0su3xu/K_Means_Unit_Test_Notebook.ipynb\n",
    "\n",
    "\n",
    "Here you will use a different dataset consisting of word-frequency distributions \n",
    "for 1,000 Twitter users. These Twitter users use language in very different ways,\n",
    "and were classified by hand according to the criteria:\n",
    "\n",
    "0: Human, where only basic human-human communication is observed.\n",
    "\n",
    "1: Cyborg, where language is primarily borrowed from other sources\n",
    "(e.g., jobs listings, classifieds postings, advertisements, etc...).\n",
    "\n",
    "2: Robot, where language is formulaically derived from unrelated sources\n",
    "(e.g., weather/seismology, police/fire event logs, etc...).\n",
    "\n",
    "3: Spammer, where language is replicated to high multiplicity\n",
    "(e.g., celebrity obsessions, personal promotion, etc... )\n",
    "\n",
    "Check out the preprints of  recent research,\n",
    "which spawned this dataset:\n",
    "\n",
    "http://arxiv.org/abs/1505.04342\n",
    "http://arxiv.org/abs/1508.01843\n",
    "\n",
    "The main data lie in the accompanying file:\n",
    "\n",
    "topUsers_Apr-Jul_2014_1000-words.txt located at:\n",
    "        https://www.dropbox.com/s/6129k2urvbvobkr/topUsers_Apr-Jul_2014_1000-words.txt?dl=0\n",
    "\n",
    "and are of the form:\n",
    "\n",
    "USERID,CODE,TOTAL,WORD1_COUNT,WORD2_COUNT,...\n",
    ".\n",
    ".\n",
    "\n",
    "where\n",
    "\n",
    "USERID = unique user identifier\n",
    "CODE = 0/1/2/3 class code\n",
    "TOTAL = sum of the word counts\n",
    "\n",
    "Using this data, you will implement a 1000-dimensional K-means algorithm in MrJob on the users\n",
    "by their 1000-dimensional word stripes/vectors using several \n",
    "centroid initializations and values of K.\n",
    "\n",
    "Note that each \"point\" is a user as represented by 1000 words, and that\n",
    "word-frequency distributions are generally heavy-tailed power-laws\n",
    "(often called Zipf distributions), and are very rare in the larger class\n",
    "of discrete, random distributions. For each user you will have to normalize\n",
    "by its \"TOTAL\" column. Try several parameterizations and initializations:\n",
    "\n",
    "(A) K=4 uniform random centroid-distributions over the 1000 words (generate 1000 random numbers and normalize the vectors)\n",
    "(B) K=2 perturbation-centroids, randomly perturbed from the aggregated (user-wide) distribution \n",
    "(C) K=4 perturbation-centroids, randomly perturbed from the aggregated (user-wide) distribution \n",
    "(D) K=4 \"trained\" centroids, determined by the sums across the classes. Use use the \n",
    "(row-normalized) class-level aggregates as 'trained' starting centroids (i.e., the training is already done for you!).\n",
    "Note that you do not have to compute the aggregated distribution or the \n",
    "class-aggregated distributions, which are rows in the auxiliary file:\n",
    "\n",
    "topUsers_Apr-Jul_2014_1000-words_summaries.txt located at:\n",
    "   https://www.dropbox.com/s/w4oklbsoqefou3b/topUsers_Apr-Jul_2014_1000-words_summaries.txt?dl=0\n",
    "\n",
    "\n",
    "Row 1: Words\n",
    "Row 2: Aggregated distribution across all classes\n",
    "Row 3-6 class-aggregated distributions for clases 0-3\n",
    "For (A),  we select 4 users randomly from a uniform distribution [1,...,1,000]\n",
    "For (B), (C), and (D)  you will have to use data from the auxiliary file: \n",
    "\n",
    "topUsers_Apr-Jul_2014_1000-words_summaries.txt\n",
    "\n",
    "This file contains 5 special word-frequency distributions:\n",
    "\n",
    "(1) The 1000-user-wide aggregate, which you will perturb for initializations\n",
    "in parts (B) and (C), and\n",
    "\n",
    "(2-5) The 4 class-level aggregates for each of the user-type classes (0/1/2/3)\n",
    "\n",
    "\n",
    "In parts (B) and (C), you will have to perturb the 1000-user aggregate \n",
    "(after initially normalizing by its sum, which is also provided).\n",
    "So if in (B) you want to create 2 perturbations of the aggregate, start\n",
    "with (1), normalize, and generate 1000 random numbers uniformly \n",
    "from the unit interval (0,1) twice (for two centroids), using:\n",
    "\n",
    "from numpy import random\n",
    "numbers = random.sample(1000)\n",
    "\n",
    "Take these 1000 numbers and add them (component-wise) to the 1000-user aggregate,\n",
    "and then renormalize to obtain one of your aggregate-perturbed initial centroids.\n",
    "\n",
    "\n",
    "###################################################################################\n",
    "## Geneate random initial centroids around the global aggregate\n",
    "## Part (B) and (C) of this question\n",
    "###################################################################################\n",
    "def startCentroidsBC(k):\n",
    "    counter = 0\n",
    "    for line in open(\"topUsers_Apr-Jul_2014_1000-words_summaries.txt\").readlines():\n",
    "        if counter == 2:        \n",
    "            data = re.split(\",\",line)\n",
    "            globalAggregate = [float(data[i+3])/float(data[2]) for i in range(1000)]\n",
    "        counter += 1\n",
    "    ## perturb the global aggregate for the four initializations    \n",
    "    centroids = []\n",
    "    for i in range(k):\n",
    "        rndpoints = random.sample(1000)\n",
    "        peturpoints = [rndpoints[n]/10+globalAggregate[n] for n in range(1000)]\n",
    "        centroids.append(peturpoints)\n",
    "        total = 0\n",
    "        for j in range(len(centroids[i])):\n",
    "            total += centroids[i][j]\n",
    "        for j in range(len(centroids[i])):\n",
    "            centroids[i][j] = centroids[i][j]/total\n",
    "    return centroids\n",
    "\n",
    "\n",
    "\n",
    "——\n",
    "For experiments A, B, C and D and iterate until a threshold (try 0.001) is reached.\n",
    "After convergence, print out a summary of the classes present in each cluster.\n",
    "In particular, report the composition as measured by the total\n",
    "portion of each class type (0-3) contained in each cluster,\n",
    "and discuss your findings and any differences in outcomes across parts A-D.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# HW 2.7: (OPTIONAL)    Logfile clean up\n",
    "\n",
    "This exercise workes  Microsoft logfiles data. The logfiles are described are located at:\n",
    "\n",
    "https://kdd.ics.uci.edu/databases/msweb/msweb.html\n",
    "http://archive.ics.uci.edu/ml/machine-learning-databases/anonymous/\n",
    "\n",
    "This dataset records which areas/pages (Vroots) of www.microsoft.com each user visited in a one-week timeframe in Feburary 1998.\n",
    "\n",
    " Here, you must preprocess the data on a single node (i.e., not on a cluster of nodes) from the format:\n",
    "\n",
    "C,\"10001\",10001   #Visitor id 10001\n",
    "V,1000,1          #Visit by Visitor 10001 to page id 1000\n",
    "V,1001,1          #Visit by Visitor 10001 to page id 1001\n",
    "V,1002,1          #Visit by Visitor 10001 to page id 1002\n",
    "C,\"10002\",10002   #Visitor id 10001\n",
    "V\n",
    "Note: #denotes comments\n",
    "to the format:\n",
    "\n",
    "V,1000,1,C, 10001\n",
    "V,1001,1,C, 10001\n",
    "V,1002,1,C, 10001\n",
    "\n",
    "Write the python code to accomplish this. \n",
    "\n",
    "2.7.1    Explain you can not do this  clean up on one machine.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW 2.8:  Find the 5 most frequently visited pages\n",
    "\n",
    "Find the 5 most frequently visited pages using MrJob from the output of HW2.7 (i.e., transfromed log file).\n",
    "\n",
    "WARNING: per-step jobconf has bugs that affect Total sorts/partitions etc.\n",
    "For MRJob,  Sort, partition code via the MRJob config does NOT work in local mode (known bug/feature which I believe has not been fixed as of June 2016). \n",
    "So you will need to run in the cloud (e.g.  in AWS).\n",
    "It's issue #616 in github:  \"Inline and Local modes should support per-step jobconf #616\".  https://github.com/Yelp/mrjob/issues/616\n",
    "To overcome this issue run your MRJob jobs on the cloud using -r hadoop or -r emr:\n",
    "\n",
    "       #!python MostFrequentVisits.py -r hadoop anonymous-msweb_converted.data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
